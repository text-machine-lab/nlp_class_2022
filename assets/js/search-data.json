{
  
    
        "post0": {
            "title": "Final Project",
            "content": "For your final project, you will need to extend your HW5 to produce a high-quality sequence-to-sequence system. This could be machine translation, multilingual machine translation, summarization, dialogue generation, code generation, or another seq-to-seq task of your choice. We provide you with a list of recommended datasets at the end of this document. . We strongly recommend to work on the final project in pairs. Both project collaborators need to make equal contributions to the codebase. You need to declare your team (or that you work on the project indivitually) in your design document on Apr 12. . Optionally, you can propose a different final project. In this case, you need to discuss it with a TA before Apr 12. You need to provide a task description, dataset, metrics, and what codebase you will use for it. . Your project should contain the following elements: . Unlike HW5, your model should contain a pre-trained component, which could be a pre-trained encoder or a pre-trained encoder-decoder model. | You will need to compare the performance of a seq-to-seq model trained from scratch with your model that includes a pre-trained component. You don’t need to train the model until convergence and can use an early checkpoint (200-300 iterations) to perform this comparison. | You should have a live demo of your model, i.e., a live interface where we would be able to enter input and view the output. You can use streamlit or gradio to create the demo. | Any extra features/ideas implemented (such as back-translation, new decoding methods, data augmentation, ensembling models, using model predictions in a non-standard way, …, ask TA for more ideas) are a big plus and would count for extra points. | Tips: . Pay attention to your cloud credits. Not planning your compute utilization in advance or not tracking it can leave you without access to a cloud GPU (and DAN417 machines have a lot of issues). For example: Do not spend 12 GPU hours verifying your HW5 works. Try to squeeze it into one or two hours. | If you are low on credits, plan accordingly and try to minimize the time you use the cloud. Test your code on your laptop before deploying it to the cloud. | . | Ask questions if you are stuck. We expect you to make progress every week. If you are struggling with the same bug or facing the wall for a day, please seek help. Ask questions in #discussion or DM a TA. Arranging one-on-one meetings is also possible, but we have a limited capacity for that. | . Milestones . Every class starting next week will be splitted into two parts: lecture and final project work. Each demonstration / document submission should be done on Tuesday during the final project work section of the class. You should do this in person, zoom submissions are possible only given a sufficient justificatoin and should be appproved a day before the class. . Apr 12: Design document. A half-page summary of what you are planning to do. Google docs format with comments enabled. This document should contain Who is working on the project (one or two people). | A link to a GitHub repository of the project. | Your dataset (or datasets) description i.e., task, language, size, source of the dataset. | What pre-trained model checkpoint you are planning to use, its architecture, its pre-training objective, language(s) it was pre-trained on. | How you are going to use this model for your task: do you need to add any extra layers / a new decoder. | The metric for model evaluation. It should be reasonable and well-suited for the task. For example, accuracy is not a good metric for machine translation. BLEU is better. At the same time, BLEU is not a good metric for summarization or code generation. | How you are planning to sample from your model at test time: greedy search, beam search, top-k sampling, etc. | . | Apr 12 (same day as design document): demonstrate that your HW5 works. I.e., you are able to train your model and achieve BLEU &gt; 10 (an hour of training should be enough). | Apr 19: Demonstrate that your training script and model (with integrated pre-trained component) works. This means that the data is pre-processed and batched correctly, | the training loop does not fail because of a bug and evaluation produces the metrics. | you don’t have to have a properly trained network at this point. Just make sure you can complete 10 training iterations and an evaluation. Showing that the model can perfectly fit 100 training examples (training accuracy &gt; 99% without regularization) would be a great plus and can give you extra points for the final project. | . | Apr 26: Demonstrate that your model learns the task. Your evaluation metrics should show significantly better performance than a random untrained model. And your model should outperform a model without a pre-trained component. At this point, you should have a trained checkpoint of the model that might perform suboptimally, but reasonably well. After that, you can focus on your presentation, writeup and demo code. | . | May 3: Final presentations with a live demo of your model. A half-page project report that describes what you have learned and the contributions of each person who worked on the project. | After May 3: Writeup submission. A 4-page conference paper-style writeup with introduction, related work, methods, and results sections. More details on that soon. | You will be evaluated on: . The quality of the presentation the slides are simple, clear, and convey the information you want, they do not contain a lot of visual clutter | the presentation is well-researched, presenters know what | a good balance between technical aspects and | . | The quality of the demo: a nice Streamlit or Gradio demo is expected. | a Jupyter or command-line demo is a big minus, but better than nothing | . | The quality of the model: Is it able to solve the task at all? | Does it perform better than the model without a pre-trained component? | How does it compare to the other models trained on this dataset? | . | The quality of the codebase: Does it contain README.md which explains a) what this project is about b) how to train the network and how to start the demo. | Contains up-to-date requirements.txt with all dependencies of your project. Every single thing you had to pip-install should be there. | Do README.md instructions actually work and we can run your code without errors. | Is it possible to run your code on a machine without a GPU? | Does your code follow PEP-8? Feel free to use auto-formatting tools such as Black. | . | The quality of the writeup Does your writeup clearly presents the problem you were trying to solve (e.g., , relevant related work you drew on | . | . Recommended datasets . Before deciding to work on a dataset (milestone 1) make sure it’s not too large and fits on your disk. You additionally need to analyze if the texts are not too large (» 512 tokens). If they are too big, you need to figure out how to trim it or not to work on the dataset at all. If you really want to work with large texts, ask TA for tips (before milestone 1). Note, that if the dataset is so large that one epoch takes more than some reasonable time (wastes your cloud credits) and you are sure this is not a bug (e.g. GPU utilization is very close to 80-100%), you can train for less than one epoch, given that the final model perfomance is reasonable. . Translation: . https://huggingface.co/datasets/code_x_glue_tt_text_to_text | . Multilingual translation . hard task, consult with a TA . https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt | Feel free to find other translation datasets, but consult with a TA about them | . Summarization: . https://huggingface.co/datasets/cnn_dailymail | https://huggingface.co/datasets/gigaword | https://huggingface.co/datasets/orange_sum | https://huggingface.co/datasets/ai4bharat/IndicSentenceSummarization | https://huggingface.co/datasets/xsum | https://huggingface.co/datasets/gem | https://huggingface.co/datasets/reddit | . Long-document summarization . hard task, consult with a TA . https://huggingface.co/datasets/ccdv/arxiv-summarization | https://huggingface.co/datasets/ccdv/pubmed-summarization | https://huggingface.co/datasets/ccdv/govreport-summarization | . Dialogue generation: . https://huggingface.co/datasets/conv_ai_2 | . Code generation (or the opposite task of code explanation): . https://huggingface.co/datasets/code_x_glue_tc_text_to_code | . Text to SQL (or SQL to text): . https://huggingface.co/datasets/spider | . Math problems . very hard task, consult with a TA . https://huggingface.co/datasets/competition_math | .",
            "url": "https://text-machine-lab.github.io/nlp_class_2022/markdown/2022/04/06/Final-project-instructions.html",
            "relUrl": "/markdown/2022/04/06/Final-project-instructions.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Homework 5 Instructions",
            "content": "Homework 5. Transformer machine translation model . You can download the code for the homework here. . Do not modify code outside # YOUR CODE STARTS HERE unless explicitly allowed by a TA. Failure to follow this will lead to failing the homework. . In this homework, we will finish Transformer implementation and train a machine translation model. . We have tested this code with a cleaned version of the WMT-14 English-German dataset (stas/wmt14-en-de-pre-processed). It is a medium-size dataset that contains 802.743 sentence pairs which are enough to train a decent translation system. . If you want to use a different language pair, please ask TA for a dataset recommendation (it can be hard to find a reasonably sized dataset, 10Mb is not enough and 20Gb is just hard to work with). If you want to use your own dataset, inform TA about it too. . 1. Implement Cross-Attention, Decoder Layer, and Transformer Encoder-Decoder Model . Start with notebooks/01_transformer_decoder.ipynb. It contains guides and instructions to implementing cross-attention within a multi-head attention class. You will need to implement the rest of the Transformer architecture, including TransformerDecoderLayer and TransformerEncoderDecoderModel.Your implementation should pass all of the tests. . There are 8 coding tasks in this part. . 2. Train tokenizers for your dataset . Go to cli/create_tokenizer and complete the script. Then train a tokenizer. . Command example: . python cli/create_tokenizer.py --dataset_name stas/wmt14-en-de-pre-processed --dataset_config ende --vocab_size 32_000 --save_dir en_de_output_dir --source_lang en --target_lang de . Because the dataset is large, this can take 5 minutes or more. Start worrying if it does not finish after 1 hour. . 3. Train a Transformer Machine Translation Model . Open notebooks/02_get_familiar_with_train.ipynb and read it carefully. It will guide you through the script and help you to solve 3 coding tasks and 3 inline questions. . After finishing all of the tasks is time to train a model. . In the beginning, try out a very small (2 layers) model in --debug mode to make sure the script can finish without failing. Then, find the largest batch size your GPU memory can fit and the largest learning rate that does not cause your model to diverge. Also feel free to play with any other hyperparameters, such as model size learning rate schedule, regularization, and so on. . When selecting the number of epochs, remember that the dataset is big. 1 epoch can take 6 hours or even more even on capable hardware. However, because the dataset is so large, 1 epoch might be enough to get a model that performs ok(-ish) with a BLEU score of around 20. Alternatively to the number of epochs, you can provide --max_train_steps. . Here’s an example script that works ok(-ish) . python cli/train.py --dataset_name stas/wmt14-en-de-pre-processed --dataset_config ende --source_lang en --target_lang de --output_dir en_de_output_dir --batch_size 64 --num_warmup_steps 5000 --learning_rate 3e-4 --num_train_epochs 1 --eval_every 5000 . 4. Try out your model . After training, use notebooks/03_interact.ipynb to translate texts using your model. This part is evaluated and (we hope) might be rewarding, as the final system should produce reasonable translations, please do not skip it. . Hyperparameters . Feel free to change hyperparameters, but remember that: . It takes hours to train a single model using a GPU. Meaning you need to start training at least a day or two before the deadline | Meaning you won’t be able to play with hyperparameters a lot | Meaning you can’t use your laptop (Core i9 laptop needs more than 100hours to train the model) | Meaning you need to learn how to work with a GPU server. | . | Smaller models are faster to train until convergence, but larger models sometimes can reach the same loss in fewer steps and less time. | Try to maximize your batch size and fill all of the GPU memory. Batch size should be at least 8 and preferably around 64 or even more. | If you see an out-of-memory error, probably your batch size or max_length or your network parameters are too big. Do not reduce max_length beyond 128. | To reduce memory consumption, you can use Adafactor optimizer instead of Adam. Here is the documentation on how to use it | . | Keep all of your shapes and sizes divisible by 8, this makes the GPUs a bit more efficient. | You can use an empirical scaling law to estimate your learning rate lr = 0.003239 - 0.0001395 log(N). Here, N is the number of parameters of your model, excluding embeddings (should be around 1-100M if you are using something like a small or base transformer). | Your final model will be evaluated based on its BLEU score. | . Finally, run cli/train.py and provide your selected hyperparameters to it. Save your model to output_dir. . Monitor your model performance while it is training in WadnB. This is literally the main purpose of this tool. If the model is not improving at all or is diverging, look up our “my model does not converge” checklist from Lecture 3. At the same time, if you get a very high test accuracy, your model might be cheating and your causal masking or data preprocessing is not implemented correctly. To help you understand what the correct training loss plot and eval perplexity/accuracy should look like, we will post a couple of images in Slack. Your runs will most probably look different, because of different hyperparameters, but they should not be extremely different. . To interrupt the script, press CTRL+C . You can download your model and tokenizer from the server to your laptop using scp command-line tool (how to use it). . Connecting to Google Cloud . Please use Google Cloud for this homework. . Follow this Stanford tutorial on how to connect to Google Cloud. Feel free to ask questions about google cloud in #discussion channel in Slack (but do not post your passwords or access tokens there or anywhere else publically, including your github). . If you are unfamiliar with SSH, look up this tutorial and if you are using Windows, this one (Windows 10 native SSH) or this one (PUTTY). . How To Keep Running Commands After SSH Session Disconnection . It takes hours to run training on a GPU. However, if you would just run python cli/train.py &lt;blahblah&gt; in your SSH session and then disconnect, your script will automatically be shutdown by the system. To avoid this, you can use tmux sessions, you can also use screen, if you are familiar with it, but do not use nohup as it is not flexible enough for our purposes. . How to sync your laptop code and server code . Use Github and Git for that. These are essential professional instruments and you will need to learn them at some point anyway. Create a private repository for your homework (your grade will be lowered, if the repository is public). And use git commands to synchronize your code. You will mostly only need these ones: git commit, git push, git clone, and git pull . . If you are unfamiliar with Git and Github: read this tutorial. . Git and GitHub are extremely popular. If you see an error — google it! You will find the answer way quicker than contacting a TA. But feel free to ask you questions in Slack too, especially if you do not understand the answer from Google. . If you have trouble with understanding git, please contact a TA. You can use scp command to sync your code but it can cause losing your changes if you are not careful. We advise not to use scp to sync your code, but only to use it for large file upload/download. . Setting up the environment . We strongly recommend using a code editor like VSCode or PyCharm. Jupyter Lab or Spyder are jupyter-notebook-first tools and are not well-suited to work with regular python modules. Here is a good tutorial on how to setup VSCode for Python. Both of them also support jupyter notebooks, you just need to specify which jupyter kernel you want to use (most probably its nlp_class). For VSCode you may want to additionally install a markdown extention to render files like this README.md. . You will be developing the package that is located inside transformer_mt. In order to be able to import this package from the notebooks, training script, and anywhere else, you need to . Note that even though this homework is similar to language modeling homework, the package name is different. It is transformer_mt instead of transformer_lm. If you see any references to transformer_lm contact a TA so that they would fix it. . If you are working in a new environment (for example, a server), you need to create a new conda environment (conda create -n nlp_class python=3.7). | Activate your python environment (e.g., conda activate nlp_class). | Go to the homework directory that contains setup.py file (the same directory this README.md is in). | Install the package using the command pip install --editable .. It should download all of the dependencies and install your module. | If you are on a GPU machine, you need to install a GPU version of PyTorch. To do that, first, check what CUDA version your server has with nvidia-smi. If your CUDA version is below 10.2, don’t use this server | If your CUDA version is below 11, run pip install torch | If your CUDA version is 11.X run, pip install torch==1.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html | Check that pytorch-GPU works via python -c &quot;import torch; print(torch.cuda.is_available())&quot;. If it returns False, reinstall pytorch via one of the above commands (usually this helps), if it doesn’t help, describe your problem in #discussion. | If you are using 30XX, A100 or A6000 GPU, you have to use CUDA 11.3 and above. | | Explaining pip install -e . command: pip is the python package manager. install is the pip command to install the module. -e is the argument that says to install the module in editable mode which allows you to edit the package and import from it right away without the need to reinstall it. The final argument is . which says “the package is located in the current directory”. . Submitting this homework . NOTE: Do not add model.pt and other large files to your git repository. . Restart your .ipynb notebooks and execute them top-to-bottom via the “Restart and run all” button. Not executed notebooks or the notebooks with the cells not executed in order will receive 0 points. | If you are using GitHub, add github.com/guitaricet and github.com/NamrataRShivagunde to your repository collaborators, so we could access your code. Push your last changes (including the executed notebooks) and submit the link to your GitHub repository to the Blackboard. | If you are not using GitHub, delete output_dir (or move its contents somewhere else if you want to reuse them later) and wandb directories. Zip this directory and submit the archive to the Blackboard. | Submit a link to your best wandb run to the Blackboard too. You will be evaluated based on the BLEU score of your model. Make sure your wandb project is public. |",
            "url": "https://text-machine-lab.github.io/nlp_class_2022/markdown/2022/03/17/Homework-5-instructions.html",
            "relUrl": "/markdown/2022/03/17/Homework-5-instructions.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Setting up the environment",
            "content": "If you already have anaconda/conda/miniconda/conda-forge, proceed to step 7. . Step 1. Download conda installer. . wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh . NOTE 1a: if this fails with “wget: command not found” install wget via “sudo apt install wget” (Ubuntu) or “brew install wget” (MacOS). Then try to do step 1 again. . NOTE 1b: alternatively, you can just follow the link above using your browser and download the file this way. . Step 2. Install miniconda. . bash ~/miniconda.sh . Step 3. Read and accept all licences. Step 4. When asked where to install conda, just press enter to install it in the default directory. Step 5. IMPORTANT: Then it will ask you “Do you wish the installer to initialize Miniconda3”. Type “yes” and press enter. Step 6. IMPORTANT: Restart your terminal session – open and close your terminal window. . NOTE: If you don’t want conda to start base envorinment in every terminal you open, you can execute conda config --set auto_activate_base false. . Step 7. Create a new environment nlp_class with python 3.7. . conda create --name nlp_class python=3.7 . NOTE: If you have conda not found error, it might mean you skipped step 5 and now need to add conda to your PATH manually. It it easy to do, here’s a solution. . Step 8. Activate this environment. . conda activate nlp_class . Step 9. Install required libraries. . (Updated Jan 25, added ipywidgets that is required for datasets widgets to work inside jupyter lab.) . python -m pip install jupyterlab torch transformers datasets scikit-learn ipywidgets . Step 10. Install a new kernel to a jupyter lab. . python -m ipykernel install --name nlp_class --user . (Updated Jan 27, added --user option to the commend which resolves /usr/local/share permission denied issue some students had) . NOTE: If step 10 fails, ask Vlad for help. . Step 11. Launch jupyter lab. . jupyter lab . This should open a browser tab with Jupyter Lab running. . Step 12. On the right, you will see a couple of available notebook enviornments under “Notebook” title. Select “nlp_class” and create a new notebook. . Step 13. Make sure all imports work. To do this execute the following code. . import torch import sklearn import transformers import datasets . If this fails, ask Vlad for help. . Now you should be good to go. If you are wondering why we need conda and environments, you can read this blogpost. .",
            "url": "https://text-machine-lab.github.io/nlp_class_2022/markdown/2022/01/18/Setting-up-the-environment.html",
            "relUrl": "/markdown/2022/01/18/Setting-up-the-environment.html",
            "date": " • Jan 18, 2022"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Course Schedule",
          "content": "Date Lecture Topic Slides Homework Quiz . Jan 18 | Intro to NLP | lecture 1, GPT-3 demo | Setting up the environment |   | . Jan 25 | Lexical embeddings | ML review, lecture 2 | Sentiment analysis with logistic regression | quiz 1 | . Feb 1 | Neural networks 101 | lecture 3 | Neural networks from scratch | quiz 2 | . Feb 8 | Neural networks 201 | lecture 4 | Fully-connected neural networks for text classification | quiz 3 | . Feb 15 | Attention mechanism | lecture 5, notes | Language modeling | quiz 4 | . Feb 22 | No Class, Monday schedule |   |   |   | . Mar 1 | Sequence to sequence networks | lecture 6 |   | quiz 5 | . Mar 8 | No Class, Spring break |   |   |   | . Mar 15 | Transfer Learning in NLP | lecture 7 | Machine Translation |   | . Mar 22 | Guest Lecture, Alexey Romanov, Neural Ranking |   |   | quiz 6 | . Mar 29 | Pre-training VariantsTopics: Knowledge Representation in BERT | lecture 8lecture 8b | BERT for classification, BERT for QA (optional, extra points) |   | . Apr 5 | Effect of scale in NLP |   | Research paper presentations |   | . Apr 12 | Guest Lecture, Mikhail Galkin, Graph neural networks |   | Supervised project work |   | . Apr 19 | Guest Lecture, Albert Webson, Multitask Prompted Training |   | Supervised project work |   | . Apr 26 | Guest Lecture / Topics |   | Supervised project work |   | . May 3 | Presentations |   |   |   | . Homeworks are due at midnight on the day before the next lecture. | Quizzes are due immediately before the next lecture. | Homeworks must be submitted via Blackboard; you must submit a PDF of your homework and a link to a Github repository with your code. | .",
          "url": "https://text-machine-lab.github.io/nlp_class_2022/schedule/",
          "relUrl": "/schedule/",
          "date": ""
      }
      
  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://text-machine-lab.github.io/nlp_class_2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}